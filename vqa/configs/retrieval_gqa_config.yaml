# For example:
multiprocessing: False
path_pretrained_models: './pretrained_models'
dataset:
    data_path: 'data'
blip_v2_model_type: blip2-flan-t5-xl  # Change to blip2-flan-t5-xl for smaller GPUs
blip_half_precision: True
# Add more changes here, following the same format as base_config.yaml
codex:
    temperature: 0.                                 # Temperature for Codex. (Almost) deterministic if 0
    best_of: 1                                      # Number of tries to choose from. Use when temperature > 0
    max_tokens: 512                                 # Maximum number of tokens to generate for Codex
    prompt: ./prompts/eval/retrieval_gqa.prompt     # Codex prompt file, which defines the API. If you use a Chat-based model (3.5/4) try ./prompts/chatapi.prompt (doesn't support video for now due to token limits)
    model: gpt-3.5-turbo-0613                       # Codex model to use. [code-davinci-002, gpt-3.5-turbo, gpt-4]